{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature making after image screening\n",
    "\n",
    "This notebook is a continuation to (1) and it should take the locks that are accepted as locks to produce a file with features. The features file this notebook produces, should have one row for every lock of the participant, with all the possible features we can make for him/her. The lock files can be found in\n",
    "\n",
    "    'data/Main collection - processed/',\n",
    "    \n",
    "and the new files will be saved in\n",
    "\n",
    "    'data/Main collection - features/'\n",
    "    \n",
    "All locks for a person will be used, and all the 916 will be computed for each lock. Once again, the folder names, of each of the participants for whom we have locks are:\n",
    "\n",
    "    'Giorgos Mon 19',\n",
    "    'the remaining names have been removed to protect the privacy of the participants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in the directory is 80, and its first 5 files are:\n",
      "Giorgos_M19_f__0.csv,\n",
      "Giorgos_M19_f__1.csv,\n",
      "Giorgos_M19_f__10.csv,\n",
      "Giorgos_M19_f__11.csv,\n",
      "Giorgos_M19_f__12.csv.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import euclidean\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# specify the place from where we will use the data\n",
    "source_path_rel = 'data/Main collection - processed/'\n",
    "\n",
    "# specify the place where we will make the new file\n",
    "target_path_rel = 'data/Main collection - features/'\n",
    "\n",
    "# specify the subject that we are running the process for\n",
    "subject = 'Giorgos Mon 19'\n",
    "\n",
    "# get the path that contains the participants\n",
    "save_path = target_path_rel + subject + '.csv'\n",
    "\n",
    "# get the path that contains the participants\n",
    "source_path = source_path_rel + subject + '/'\n",
    "\n",
    "# get the files in the path that contain the names of the participants\n",
    "lock_file_names = os.listdir(source_path)\n",
    "\n",
    "# remove the logs file\n",
    "lock_file_names.remove('logs')\n",
    "\n",
    "print(\"The number of files in the directory is {}, and its first \\\n",
    "5 files are:\\n{},\\n{},\\n{},\\n{},\\n{}.\".format(len(lock_file_names), \n",
    "                                        *lock_file_names[:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING OPTIONS\n",
    "np.set_printoptions(threshold=40)\n",
    "pd.set_option('display.max_rows', 280)\n",
    "pd.set_option('display.max_columns', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Folder Names to Natural Order\n",
    "\n",
    "There is no particular reason to sort the list according the order that makes sense, just to give some more high level control of the flow of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in the directory is 80, and its first 5 files are:\n",
      "Giorgos_M19_f__0.csv,\n",
      "Giorgos_M19_f__1.csv,\n",
      "Giorgos_M19_f__2.csv,\n",
      "Giorgos_M19_f__4.csv,\n",
      "Giorgos_M19_f__5.csv.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    \"\"\" These two functions help to list the files in natual order \"\"\"\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    \"\"\"\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# sort the files according to natural order\n",
    "lock_file_names.sort(key=natural_keys)\n",
    "\n",
    "print(\"The number of files in the directory is {}, and its first \\\n",
    "5 files are:\\n{},\\n{},\\n{},\\n{},\\n{}.\".format(len(lock_file_names), \n",
    "                                        *lock_file_names[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Points of Interest\n",
    "\n",
    "Findding the PoIs is maybe the most important part of this notebook. These points will be used, crucially, to make most of the features. The method though, given that this is not the subject of this project, is not very robust. For that reason, in the previous notebook we have tested and made sure that the PoIs can be found adequately with the algorithms that are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCE(df, idxD, idxB, idxF, verbose=False):\n",
    "    \"\"\" Finds the C and E indices, based on B, D and F points. \"\"\"\n",
    "    \n",
    "    # find index C\n",
    "    idxC = df['x'][idxB:idxD].idxmin()\n",
    "    \n",
    "    # find index E\n",
    "    idxE = df['x'][idxD:idxF].idxmax()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The C index is {}.\".format(idxC))\n",
    "        print(\"The E index is {}.\".format(idxE))\n",
    "        \n",
    "    return idxC, idxE\n",
    "\n",
    "\n",
    "def findF(df, idxG, idxD, verbose=False):\n",
    "    \"\"\" Finds the index of point F, based on the gradual change of the movement\n",
    "    (capture by percenting change). The calculations happen on around one third \n",
    "    of the shape.\"\"\"\n",
    "    \n",
    "    # make a dataframe that records the change in x and y values in percentages\n",
    "    dpct = df['x'][idxD:].pct_change(periods=1, fill_method='pad', limit=1, freq=None)\n",
    "    \n",
    "    # fill the NaN values\n",
    "    dpct = dpct.fillna(value=0)\n",
    "        \n",
    "    # find the first index, coming from D, that has negative change\n",
    "    idxFirstNeg = dpct[dpct<0].head(1).index.values[0]\n",
    "    \n",
    "    # initialize index F\n",
    "    idxF = df['x'][idxFirstNeg:idxG].idxmin()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The F index is {}.\".format(idxF))\n",
    "        \n",
    "    return idxF\n",
    "\n",
    "\n",
    "def findB(df, idxA, idxD, verbose=False):\n",
    "    \"\"\" Finds the index of point B, based on the gradual change of the movement\n",
    "    (capture by percenting change). The calculations happen on around one third \n",
    "    of the shape.\"\"\"\n",
    "    \n",
    "    # make a dataframe that records the change in x and y values in percentages\n",
    "    dpct = df['x'][:idxD].pct_change(periods=1, fill_method='pad', limit=1, freq=None)\n",
    "    \n",
    "    # fill the NaN values\n",
    "    dpct = dpct.fillna(value=0)\n",
    "    \n",
    "    # find the first index, in the range from A to D, that has negative change\n",
    "    idxLastNeg = dpct[dpct<0].tail(1).index.values[0]\n",
    "    \n",
    "    # initialize index F\n",
    "    idxB = df['x'][idxA:idxLastNeg].idxmax()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The B index is {}.\".format(idxB))\n",
    "        \n",
    "    return idxB\n",
    "\n",
    "\n",
    "def findG(df, verbose=False):\n",
    "    \"\"\" Finds the index of point G of the lock, by the maximun values of its two\n",
    "    columns x and y. \"\"\"\n",
    "  \n",
    "    # initialize dummy dataframe\n",
    "    dummy = pd.DataFrame(columns=['y_rev', 'x and y_rev'])\n",
    "  \n",
    "    # find the max value in the y axis\n",
    "    max_y = df['y'].max()\n",
    "    \n",
    "    if verbose:\n",
    "        print('The max value in y axis of the plot is {}.'.format(max_y))\n",
    "  \n",
    "    # make a reversed(y) column\n",
    "    dummy['y_rev'] = df['y'].apply(lambda x: max_y - x)\n",
    "\n",
    "    # make the column we extract G's index from\n",
    "    dummy['x and y_rev'] = df['x'].add(dummy['y_rev'])\n",
    "\n",
    "    # get the index of G\n",
    "    idxG = dummy['x and y_rev'].idxmax()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The index G is... {}.\".format(idxG))\n",
    "\n",
    "    return idxG\n",
    "\n",
    "\n",
    "def findIndices(data):\n",
    "    \"\"\" Finds the indices of the Points of Interest in the lock. This function\n",
    "    workds under the assumption that the start of the axis is near point A. \"\"\"\n",
    "    \n",
    "    # find the indexes for A and H (both bottom left A starts the shape and H ends it) and G (bottom right)\n",
    "    iA, iH, iG = 0, -1, findG(data)\n",
    "    \n",
    "    # index for D (highest point)\n",
    "    iD = data['y'].idxmax()\n",
    "    \n",
    "    # find the index for B (neck left)\n",
    "    iB = findB(data, iA, iD)\n",
    "    \n",
    "    # find the index for F (neck right)\n",
    "    iF = findF(data, iG, iD)\n",
    "    \n",
    "    # find the indices for E (circle right) and F (neck right)\n",
    "    iC, iE = findCE(data, iD, iB, iF)\n",
    "        \n",
    "    return iA, iB, iC, iD, iE, iF, iG, iH\n",
    "\n",
    "\n",
    "def findPoints(data, iA, iB, iC, iD, iE, iF, iG, iH, keepTimestamp=False):\n",
    "    \"\"\" Finds the points in the dataframe according the indices \"\"\"\n",
    "    \n",
    "    if keepTimestamp: # find ABCDEFGH while keeping 'millis' column\n",
    "        \n",
    "        A = data.iloc[iA]  # bottom left (starting point)\n",
    "        B = data.iloc[iB]  # neck left\n",
    "        C = data.iloc[iC]  # head left\n",
    "        D = data.iloc[iD]  # highest point\n",
    "        E = data.iloc[iE]  # head right\n",
    "        F = data.iloc[iF]  # neck right\n",
    "        G = data.iloc[iG]  # bottom right\n",
    "        H = data.iloc[iH]  # bottom left (ending point)\n",
    "            \n",
    "        return A, B, C, D, E, F, G, H\n",
    "        \n",
    "    else: # find ABCDEFGH while dropping 'millis' column\n",
    "        \n",
    "        A = data.iloc[iA][:3]  # bottom left (starting point)\n",
    "        B = data.iloc[iB][:3]  # neck left\n",
    "        C = data.iloc[iC][:3]  # head left\n",
    "        D = data.iloc[iD][:3]  # highest point\n",
    "        E = data.iloc[iE][:3]  # head right\n",
    "        F = data.iloc[iF][:3]  # neck right\n",
    "        G = data.iloc[iG][:3]  # bottom right\n",
    "        H = data.iloc[iH][:3]  # bottom left (ending point)\n",
    "            \n",
    "        return A, B, C, D, E, F, G, H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "\n",
    "In this step we will use the PoIs functions do create features that do or donnot depend on them. These features can be separated in three groups:\n",
    "\n",
    "1. Global features\n",
    "2. Distance Features (in-between Points of Interest)\n",
    "3. Ratios between all Distance Features\n",
    "\n",
    "The way to create all of them for each file separately, is by loading the file and computing various characteristics that we store on a python dictionary. That dictionary will be a line into the features file, every key-value pair will refer to a column and its value. Note that as we said earlier in this stage we want to drop as much features as possible to it.\n",
    "\n",
    "As far as **Global Features** go, these refer to the features that can come up by looking at the distribuitioons that can come up from the raw data, without using any semantic ques such as Points of Interest. There are 11 distributions that we take into account here, and we are computing 9 statistics for 7 of them, and 10 statistics for the remaining 4. The distributions are:\n",
    "\n",
    "    'positional values for x, y and z (raw data)'           -> 9 statistics\n",
    "    'changes in position for x, y, z and their magnitude'   -> 10 statistics (includes summation)\n",
    "    'speed with which the change in position happened'      -> 9 statistics\n",
    "\n",
    "In the case of the changes in position, it is the only place where it makes sense to sum up all the elements because that is the measure of the total distance covered in the x, y, z and magnitude which is the actual total distance. All these accound for $(3*9) + (4*10) + (4*9) = 103$ features. If we add the total time taken to make the shape, we have $104$ Global Features.\n",
    "\n",
    "The naming convention for every feature goes like this: **'axis name' + 'group it belongs to' + '_' + 'statistic'**. The axis names can be x, y, z, millis or mag (for magnitude). The group it belongs to is CamelCased and is one of the groups mentioned above, namely 'Pos', 'PosInc' and 'Speed'. The statistics are: mean, median, std, variance, skewedness, kurtosis, min, max, range - and summation in the case of 'PosInc'.\n",
    "\n",
    "Regarding **Distance Features**, these are related with the Points of Interest Concept which is the important points that in some sense define a lock drawing. These Points are 8 as it could be seen in the screening earlier, and in this group of features we are taking into account all the possible distances in-between those 8 points, which is $8 \\choose 2$$=28$. However that is not the entire story since we can take eucliden distances and temporal distances, which makes it $28*2=56$ features.\n",
    "\n",
    "Finally, on a similar fashion with Distance Features, in the case of **Ratio Features** we are taking the 28 eucliden distances and the 28 temporal distances and we are again finding all possible ratios between them. That accoutns to $28 \\choose 2$$=378$ features, which ends up if we take both categories into account, in $756$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 2. Ratio Features (RoIs) #############################################################################\n",
    "def extractRatios(ROIs, d):\n",
    "    \"\"\" Constructs the lock features according to the Points of Interest. \"\"\"\n",
    "    \n",
    "    # start the banlist\n",
    "    banList = []\n",
    "    \n",
    "    # find the distances\n",
    "    for key1, values1 in ROIs.items():\n",
    "        \n",
    "        # update the banlist\n",
    "        banList.append(key1)\n",
    "        \n",
    "        for key2, values2 in ROIs.items():\n",
    "            \n",
    "            # abort the loop if the key is in banlist already\n",
    "            if key2 in banList:\n",
    "                continue\n",
    "            \n",
    "            #calculate the ratio for spatial distance of the pair at hand\n",
    "            d[key1+'|'+key2+'_xyz'] = values1['xyz'] / values2['xyz']\n",
    "            \n",
    "            #calculate the temporal distance for every pair and add it to dict\n",
    "            d[key1+'|'+key2+'_mil'] = values1['millisec'] / values2['millisec']\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def extractROIs(POIs, d):\n",
    "    \"\"\" Constructs the lock features according to the Points of Interest. \"\"\"\n",
    "    \n",
    "    # start the banlist\n",
    "    banList = []\n",
    "    \n",
    "    # start the list\n",
    "    ROIs = []\n",
    "    \n",
    "    # find the distances\n",
    "    for key1, values1 in POIs.items():\n",
    "        \n",
    "        # update the banlist\n",
    "        banList.append(key1)\n",
    "        \n",
    "        for key2, values2 in POIs.items():\n",
    "            \n",
    "            # abort the loop if the key is in banlist already\n",
    "            if key2 in banList:\n",
    "                continue\n",
    "            \n",
    "            #append the correct string\n",
    "            ROIs.append(key1+key2)\n",
    "    \n",
    "    # initialize the ROIs actual data structure with the keys we just came up\n",
    "    ROIs = {key: {'xyz': [], 'millisec': []} for key in ROIs}\n",
    "        \n",
    "    # establish the correct dictionary\n",
    "    for key, value in ROIs.items():\n",
    "\n",
    "        # add them to dict\n",
    "        value['xyz'] = d[key+'_xyz']\n",
    "        value['millisec'] = d[key+'_mil']\n",
    "            \n",
    "    return ROIs\n",
    "\n",
    "\n",
    "##### 2. Dinstance Features (PoIs) #############################################################################\n",
    "def extractDistances(POIs, d):\n",
    "    \"\"\" Constructs the lock features according to the Points of Interest. \"\"\"\n",
    "    \n",
    "    # start the banlist\n",
    "    banList = []\n",
    "    \n",
    "    # find the distances\n",
    "    for key1, values1 in POIs.items():\n",
    "        \n",
    "        # update the banlist\n",
    "        banList.append(key1)\n",
    "        \n",
    "        for key2, values2 in POIs.items():\n",
    "            \n",
    "            # abort the loop if the key is in banlist already\n",
    "            if key2 in banList:\n",
    "                continue\n",
    "            \n",
    "            #calculate the spatial distance for every pair and add to dict\n",
    "            d[key1+key2+'_xyz'] = euclidean(values1['xyz'], values2['xyz'])\n",
    "            \n",
    "            #calculate the temporal distance for every pair and add it to dict\n",
    "            d[key1+key2+'_mil'] = abs(values2['millisec'][0] - values1['millisec'][0])\n",
    "    \n",
    "    return d\n",
    "\n",
    "\n",
    "def extractPOIs(df):\n",
    "    \"\"\" Find the Points of Interest of each file. It returns a data structure of \n",
    "    a dictionary of dictionaries. The first dictionary has every point's name\n",
    "    (e.g. 'A') as keys, and the second, the corresponding index, XYZ value and \n",
    "    time. \"\"\"\n",
    "  \n",
    "    # initialize the data structure\n",
    "    POIs = {'A': None, \n",
    "            'B': None, \n",
    "            'C': None, \n",
    "            'D': None, \n",
    "            'E': None, \n",
    "            'F': None, \n",
    "            'G': None,\n",
    "            'H': None}\n",
    "  \n",
    "    # populate it with an internal dict\n",
    "    for k, v in POIs.items():\n",
    "        POIs[k] = {'index': [], 'xyz': [], 'millisec': []}\n",
    "  \n",
    "    # find the indices for all POIs\n",
    "    iA, iB, iC, iD, iE, iF, iG, iH = findIndices(df)\n",
    "    \n",
    "    # list the values that will go to POIs\n",
    "    values = findPoints(df, iA, iB, iC, iD, iE, iF, iG, iH, keepTimestamp=True)\n",
    "      \n",
    "    # list the keys, the indices, the XYZs and the millis of the dictionary\n",
    "    keys = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    indices = [iA, iB, iC, iD, iE, iF, iG, iH] # i delayed making them a list to make it more readable..\n",
    "\n",
    "    for i in range(len(keys)):\n",
    "        POIs[keys[i]]['index'].append(indices[i])\n",
    "        POIs[keys[i]]['xyz'].extend([values[i]['x'], values[i]['y'], values[i]['z']])\n",
    "        POIs[keys[i]]['millisec'].append(values[i]['millis'])\n",
    "  \n",
    "    return POIs\n",
    "\n",
    "\n",
    "##### 1. Simple Features #############################################################################\n",
    "def allMeasurements(series, tag, d):\n",
    "    \"\"\" Accepts a pandas series and does multiple statistical operations. The default way to treat \n",
    "    NaNs is to skip them. \"\"\"\n",
    "    \n",
    "    # find mean\n",
    "    d[tag+'_mean'] = series.mean()\n",
    "\n",
    "    # find median\n",
    "    d[tag+'_median'] = series.median()\n",
    "\n",
    "    # find variance (2nd moment)\n",
    "    d[tag+'_var'] = series.var(ddof=0)\n",
    "\n",
    "    # find standard deviation\n",
    "    d[tag+'_std'] = series.std(ddof=0)\n",
    "\n",
    "    # find skewness (3rd moment)\n",
    "    d[tag+'_skew'] = series.skew()\n",
    "\n",
    "    # find kurtosis (4th moment)\n",
    "    d[tag+'_kurt'] = series.kurt()\n",
    "\n",
    "    # find min\n",
    "    d[tag+'_min'] = series.min()\n",
    "\n",
    "    # find max\n",
    "    d[tag+'_max'] = series.max()\n",
    "\n",
    "    # find range (max - min)\n",
    "    d[tag+'_range'] = d[tag+'_max'] - d[tag+'_min']\n",
    "    \n",
    "    if tag.endswith('Inc'):\n",
    "            \n",
    "        # make an additional summation measurement (total distance covered)\n",
    "        d[tag+'_sum'] = series.sum()\n",
    "\n",
    "\n",
    "def remakePosGroup(df):\n",
    "    \"\"\" Remakes the three positional columns to start from 0 \"\"\"\n",
    "    \n",
    "    # find the starting values of x y and z\n",
    "    xx = df.head(1)['xPos'].values[0]\n",
    "    yy = df.head(1)['yPos'].values[0]\n",
    "    zz = df.head(1)['zPos'].values[0]\n",
    "    \n",
    "    # subtract the above values from every entry to position the lock to start from (0, 0, 0)\n",
    "    df['xPos'] = df['xPos'].sub(xx)\n",
    "    df['yPos'] = df['yPos'].sub(yy)\n",
    "    df['zPos'] = df['zPos'].sub(zz)\n",
    "    \n",
    "        \n",
    "def extractSimpleFeatures(df_raw, d):\n",
    "    \"\"\" Extracts simple features from data \"\"\"\n",
    "    \n",
    "    # make a copy of the dataframe to play\n",
    "    df = pd.DataFrame(df_raw)\n",
    "    \n",
    "    # change column names to be in the desired format for csv\n",
    "    df.rename(index=int, copy=False, columns={'x': 'xPos', 'y': 'yPos', 'z': 'zPos'}, inplace=True)\n",
    "    \n",
    "    # calculate the 3 positional increment columns and the time increments\n",
    "    df[['xPosInc', 'yPosInc', 'zPosInc', 'millisInc']] = df.sub(df.shift(1)).fillna(0)\n",
    "    \n",
    "    # calculate the change of magnitude column\n",
    "    df['magPosInc'] = np.sqrt(df[['xPosInc', 'yPosInc', 'zPosInc']].pow(2).sum(axis=1))\n",
    "    \n",
    "    # calculate the 4 speed increment columns\n",
    "    df[['xSpeed', 'ySpeed', 'zSpeed', 'magSpeed']] = \\\n",
    "     df[['xPosInc', 'yPosInc', 'zPosInc', 'magPosInc']].div(df['millisInc'], axis=0)\n",
    "        \n",
    "    # remake xPos, yPos and zPos to start from 0\n",
    "    remakePosGroup(df)\n",
    "        \n",
    "    #display(df)\n",
    "    \n",
    "    # initialize the axis that conta\n",
    "    distributions = ['xPos', 'yPos', 'zPos', \n",
    "                     'xPosInc', 'yPosInc', 'zPosInc', 'magPosInc', \n",
    "                     'xSpeed', 'ySpeed', 'zSpeed', 'magSpeed']\n",
    "    \n",
    "    for dist in distributions:\n",
    "        \n",
    "        # make all measurements for the distributions of each axis\n",
    "        allMeasurements(df[dist], dist, d)\n",
    "                    \n",
    "    # the 104th feature is the total time taken (only purely temporal feature)\n",
    "    d['millis_range'] = df.tail(1)['millis'].values[0] - df.head(1)['millis'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control appending to csv once created\n",
    "appendNewLock = False\n",
    "\n",
    "for file in lock_file_names:\n",
    "    \n",
    "    # load the file\n",
    "    data = pd.read_csv(source_path+file)\n",
    "    \n",
    "    # make a dictionary for the instance features\n",
    "    d = {}\n",
    "    \n",
    "    # 1. construct SIMPLE FEATURES\n",
    "    extractSimpleFeatures(data, d)\n",
    "    \n",
    "    # find the Points of Interest of the 3d shape\n",
    "    POIs = extractPOIs(data)\n",
    "      \n",
    "    # 2. construct DISTANCE FEATURES\n",
    "    d = extractDistances(POIs, d)\n",
    "\n",
    "    # find the Ratios of Interest of the 3d shape\n",
    "    ROIs = extractROIs(POIs, d)\n",
    "      \n",
    "    # 3. construct RATIO FEATURES\n",
    "    d = extractRatios(ROIs, d)\n",
    "    \n",
    "    # upload into a new dataframe line for easy of passing it to csv\n",
    "    d = pd.DataFrame(d, index=[0])\n",
    "    \n",
    "    #display(d)\n",
    "    \n",
    "    #break #-------------------------------------------------\n",
    "    \n",
    "    if appendNewLock:\n",
    "        # append to existing\n",
    "        d.to_csv(save_path, index=False, mode='a', header=False) \n",
    "    else:\n",
    "        # create new csv\n",
    "        d.to_csv(save_path, index=False, mode='w')\n",
    "    \n",
    "    # switch for appending instead of creating new csvs\n",
    "    appendNewLock = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns of the last file is 916.\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of columns of the last file is {}.\".format(d.columns.values.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
