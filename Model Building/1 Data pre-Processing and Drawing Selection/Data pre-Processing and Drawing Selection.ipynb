{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Lock Classification and pre-Processing.\n",
    "\n",
    "The purpose of this notebook is to select and pre-process the lock files for a single person.\n",
    "\n",
    "The paths used are:\n",
    "- to find the locks: 'data/Main collection - raw/'\n",
    "- to create the locks: 'data/' (then the folder should manualy go to Main collection - processed)\n",
    "\n",
    "To begin the process the first thing to do is to import the necessary libraries, initialize the path variables, and create the folder for whom we are creating the processed files. After that we will use some pre-processing functions before visualizing each image. During the visualization process user input is required to say if the image should be kept or dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in the directory is 93, and its first 5 files are:\n",
      "Giorgos_M19_f__0.csv,\n",
      "Giorgos_M19_f__1.csv,\n",
      "Giorgos_M19_f__10.csv,\n",
      "Giorgos_M19_f__11.csv,\n",
      "Giorgos_M19_f__12.csv.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# specify the subject that we are running the process for\n",
    "subject = 'Giorgos Mon 19'\n",
    "\n",
    "# specify saving paths for the accepted and rejected images\n",
    "img_saving_path = 'images/' + subject + '/'\n",
    "img_saving_path_accepted = img_saving_path+'accepted images/'\n",
    "img_saving_path_rejected = img_saving_path+'rejected images/'\n",
    "\n",
    "# delete the subject folder and all its contents if it exists\n",
    "shutil.rmtree(img_saving_path) if os.path.exists(img_saving_path) else None\n",
    "\n",
    "# get the files in the path that contain the names of the participants\n",
    "os.mkdir(img_saving_path)\n",
    "os.mkdir(img_saving_path_accepted)\n",
    "os.mkdir(img_saving_path_rejected)\n",
    "\n",
    "# specify the place where we will make the new file\n",
    "target_path_rel = 'data/'\n",
    "\n",
    "# get the path that contains the participants\n",
    "target_path = target_path_rel + subject + '/'\n",
    "\n",
    "# delete the subject folder and all its contents if it exists\n",
    "shutil.rmtree(target_path) if os.path.exists(target_path) else None\n",
    "    \n",
    "# get the files in the path that contain the names of the participants\n",
    "os.mkdir(target_path)\n",
    "os.mkdir(target_path+'logs/')\n",
    "\n",
    "# specify the place from where we will use the data\n",
    "source_path_rel = 'data/Main collection - raw/'\n",
    "\n",
    "# get the path that contains the participants\n",
    "source_path = source_path_rel + subject + '/'\n",
    "\n",
    "# get the files in the path that contain the names of the participants\n",
    "lock_file_names = os.listdir(source_path)\n",
    "\n",
    "print(\"The number of files in the directory is {}, and its first \\\n",
    "5 files are:\\n{},\\n{},\\n{},\\n{},\\n{}.\".format(len(lock_file_names), \n",
    "                                        *lock_file_names[:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING OPTIONS\n",
    "np.set_printoptions(threshold=40)\n",
    "pd.set_option('display.max_rows', 280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting Folder Names to Natural Order\n",
    "\n",
    "There is no particular reason to sort the list according the order that makes sense, just to give some more high level control of the flow of the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in the directory is 93, and its first 5 files are:\n",
      "Giorgos_M19_f__0.csv,\n",
      "Giorgos_M19_f__1.csv,\n",
      "Giorgos_M19_f__2.csv,\n",
      "Giorgos_M19_f__3.csv,\n",
      "Giorgos_M19_f__4.csv.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def atoi(text):\n",
    "    \"\"\" These two functions help to list the files in natual order \"\"\"\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    \"\"\"\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "# sort the files according to natural order\n",
    "lock_file_names.sort(key=natural_keys)\n",
    "\n",
    "print(\"The number of files in the directory is {}, and its first \\\n",
    "5 files are:\\n{},\\n{},\\n{},\\n{},\\n{}.\".format(len(lock_file_names), \n",
    "                                        *lock_file_names[:6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Pre-processing functions\n",
    "\n",
    "The lock selection will be in terms of the 'lockiness' of a lock and the success in calculating some Points of Interest that will be used to make features. In order to improve the number of locks that can be considered locks, and to succeed more often when calculating the PoIs, we will do some pre-processing before making features for every lock. The operations we will do will be to:\n",
    "\n",
    "1. Reset of their co-ordinates (meaning to change the center of the interaction box).\n",
    "2. Chop of the front and back tails of the lock.\n",
    "3. Remove duplicates (will help the next step).\n",
    "4. Interpolate some crazy values.\n",
    "\n",
    "The first step will help to bring the lock into a space that makes more intuitive sense, and brings the start of the axis in the front left corner of the interaction box, from the perspective of the user. The second step deals with the fact that the user sometimes delayed to make a move and we do not want to take the slow reaction time into account. Finally, the third step is because sometimes the Kinect was glitching and some times it gave some crazy values (e.g. in some rare cases the user would momentarily bring the palm closer to the kinect than the index finger and one value needs to be corrected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordinate reset\n",
    "\n",
    "The functions that deal with the co-ordinate reset are in the cell below. They use some values that refer to the optics of the Kinect v2 and they were taken from [1].\n",
    "\n",
    "[1] https://github.com/shiffman/OpenKinect-for-Processing/tree/master/OpenKinect-Processing/examples/Kinect_v2/DepthPointCloud2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "def getOffsetX(X):\n",
    "    \"\"\" Calculates the offset for x dimension according to the Kinect v2 \n",
    "    specifications. The offset is different than just mapping pixel values \n",
    "    into their real dimensions so only 0 and 511 which is the leftmost and \n",
    "    rightmost possible pixel indices \n",
    "    - X: x position of the pixel of the kinect depth image \"\"\"\n",
    "\n",
    "    if X != 0 and X != 511:\n",
    "        raise ValueError(\"X pixel needs to be set as 0 or 511 which are the \\\n",
    "leftmost and rightmost indices of the pixels of the Kinect depth image\")\n",
    "  \n",
    "    cx = 254.878 # parameter of the Kinect v2 optics\n",
    "    fx = 365.456 # parameter of the Kinect v2 optics\n",
    "    #maxX = 0     # min pixel width of the kinect v2 depth camera\n",
    "    maxZ = 4500  # max range of the kinect v2 depth camera\n",
    "    \n",
    "    return (X - cx) * maxZ / fx\n",
    "\n",
    "\n",
    "def getOffsetY(Y):\n",
    "    \"\"\" Calculates the offset for y dimension according to the Kinect v2 \n",
    "    specifications. The offset is different than just mapping pixel values \n",
    "    into their real dimensions so only 0 and 423 which is the highest and \n",
    "    lowest possible pixel indices \n",
    "    - Y: y position of the pixel of the kinect depth image \"\"\"\n",
    "\n",
    "    if Y != 0 and Y != 423:\n",
    "        raise ValueError(\"Y pixel needs to be set as 0 or 423 which are the \\\n",
    "highest and lowest indices of the pixels of the Kinect depth image\")\n",
    "  \n",
    "    cy = 205.395 # parameter of the Kinect v2 optics\n",
    "    fy = 365.456 # parameter of the Kinect v2 optics\n",
    "    #maxY = 424   # max pixel width of the kinect v2 depth camera\n",
    "    maxZ = 4500  # max range of the kinect v2 depth camera\n",
    "  \n",
    "    return (Y - cy) * maxZ / fy\n",
    "\n",
    "\n",
    "def roundUp(x):\n",
    "    \"\"\" Round a value upwards to the first decimal \"\"\"\n",
    "    return ceil(x * 10) / 10.0\n",
    "\n",
    "\n",
    "def resetXY(data, offset_Z=500, pixelX=0, pixelY=423):\n",
    "    \"\"\" Resets x and y data to bring them to the desired coordinate system. \n",
    "    - roundUpwards: rounds the offset values upwards, to the first decimal \"\"\"\n",
    "  \n",
    "    # set the offsets by the KinectData class\n",
    "    offset_X, offset_Y = getOffsetX(pixelX), getOffsetY(pixelY)\n",
    "  \n",
    "    # take the absolute value of offset_X to prepare it for addition\n",
    "    offset_X = abs(offset_X)\n",
    "\n",
    "    # reset X\n",
    "    data['x'] = data['x'].add(offset_X)\n",
    "  \n",
    "    # reset Y\n",
    "    data['y'] = data['y'].subtract(offset_Y)\n",
    "  \n",
    "    # make the negative value of Y into positive (this also flips the y axis)\n",
    "    data['y'] = data['y'].apply(lambda x: abs(x))\n",
    "  \n",
    "    # reset Z\n",
    "    data['z'] = data['z'].subtract(offset_Z)\n",
    "  \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chopping sides\n",
    "\n",
    "Also, chopping the sides will remove the response time elements which we do not want to leak into the features. Regarding the backchop, there are 56 lines that represent the closing signal, and we could maybe drop all of them because that is no shapemaking but just waiting from the side of the participant to close the signal. However we will leave a few of them (6) and drop less than 56, because usually some of them might be included at the tail end of someone's shape.\n",
    "\n",
    "Regarding the frontchop the situation is a bit more complicated. In the front tail, there are no extra points from the signal, since they are discarded from the application that created the data. The signal points are not included in the buffer that will later on become the csv. However, even when the signal is made, the user's movement doesn't start immediately. Some users take more time to respond and some less, and all these points are being recorded. So, in order to avoid keeping the points that are after signal but before the actual drawing, we will check the euclidean distance the very first point has with the ones that follow. When that distance is above 20mm we can be sure that substantial movement has occured. Also, similarly to the previous case we will not discard all the points but we will keep 3 (chosen arbitrary like the backchop case) that should capture the early movement, that presumably has started in the 20mm window. Theoretically, there is also the case that the participant will be drifting for more than 20mm before he starts the movement but these incidents should be very rare, and for not much distance since prolonged movement can be seen on the shape and has therefore been discarded earlier.\n",
    "\n",
    "In addition to that we will keep a logs file with the duplicates of a processed image, the front chop and the backchop and statistics about the front chop and the duplicates in the entire file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "def chopSides(data, tFront=None, tBack=None, flatChopBack=None, trim_break=3):\n",
    "    \"\"\" Chops the unecessary points from the sides (beginning - ending) of a \n",
    "    drawing 3D line such as the lock. It accepts:\n",
    "    - data: as a pandas dataframe of the data\n",
    "    - tFront: as a threshold value for how much difference is allowed at the \n",
    "      front side of the trace\n",
    "    - tBack: as a threshold value for how much difference is allowed at the \n",
    "      back side of the trace\n",
    "    - flatChopBack: flat chop amount from the back\n",
    "    - trim_break: the trim break factor accounts for the number of rows that \n",
    "      will be kept despite the fact that they break the threshold \"\"\"\n",
    "  \n",
    "    # raise error if two options are chosen for the back chop\n",
    "    if tBack and flatChopBack:\n",
    "        raise ValueError(\"Two backchop options are specified instead of one.\")\n",
    "  \n",
    "    # return early with a notification if all values are None\n",
    "    if tFront is None and tBack is None and flatChopBack is None:\n",
    "        print(\"All trimming values are None, no chopping was done.\")\n",
    "        return data, 0, 0\n",
    "\n",
    "    # if only flatChopBack is set to a numeric value\n",
    "    if flatChopBack and not tFront and not tBack:\n",
    "        return data[:-flatChopBack], 0, flatChopBack\n",
    "    \n",
    "    # initialize the counters\n",
    "    frontChopCounter, backChopCounter = 0, 0\n",
    "  \n",
    "    # load the threshold values to a dictionary\n",
    "    t = {'f': tFront, 'b':tBack}\n",
    "  \n",
    "    # s will control frontwards and backwards traversing\n",
    "    s = 'f' if t['f'] else ''\n",
    "    s = s+'b' if t['b'] else s\n",
    "      \n",
    "    # loop once or twice depending on the sides selection\n",
    "    for letter in s:\n",
    "          \n",
    "        # reverse the data to traverse backwards\n",
    "        data = data[::-1] if letter=='b' else data\n",
    "    \n",
    "        # hold the first line as a reference r, iloc gets first line no matter the df index\n",
    "        firstLine = data.iloc[0]\n",
    "    \n",
    "        # loop over data and break with the first incident of dissimilarity\n",
    "        for i, row in data.iloc[1:].iterrows():\n",
    "      \n",
    "            # checks if the line is NOT similar enough with the reference\n",
    "            if euclidean([row['x'], row['y'], row['z']], \n",
    "                         [firstLine['x'], firstLine['y'], firstLine['z']]) > t[letter]:\n",
    "                \n",
    "                # prevent bugs in case the trim factor is bigger than i - should be very rare\n",
    "                trim_break = 0 if i<trim_break else trim_break\n",
    "        \n",
    "                # change the i if backwards\n",
    "                i = data.shape[0]-i+trim_break if letter == 'b' else i-trim_break\n",
    "        \n",
    "                # set results to the appropriate slice of \"data\"\n",
    "                data = data[i:]\n",
    "        \n",
    "                # set the counters depending on the side the loop is for\n",
    "                frontChopCounter = i if letter == 'f' else frontChopCounter\n",
    "                backChopCounter = i if letter == 'b' else backChopCounter\n",
    "                break\n",
    "    \n",
    "        # reset indices if front chop occured\n",
    "        data.index = range(data.shape[0]) if letter=='f' else data.index\n",
    "    \n",
    "        # reverse data again if they have been reversed for the backwards loop\n",
    "        data = data[::-1] if letter=='b' else data\n",
    "  \n",
    "    if flatChopBack:\n",
    "  \n",
    "        # if there is a flatChop amount specified for the back tail slice it out \n",
    "        data = data[:-flatChopBack]\n",
    "    \n",
    "        # add flatChopBack to the variable that gets returned\n",
    "        backChopCounter = flatChopBack\n",
    "    \n",
    "    # returns the data, the amount of front and back lines chopped separately\n",
    "    return data, frontChopCounter, backChopCounter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding and Removing Duplicates\n",
    "\n",
    "In order to improve the interpolation process that follows, the points that are exact duplicates in terms of their 'x', 'y' and 'z' with their previous point will be found. Then the oldest point is kept and the earliest removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForDuplicates(data, variableNames='xyz', removeDuplicates=False):\n",
    "    \"\"\" Checks for duplicate lines in a dataframe \"\"\"\n",
    "  \n",
    "    # sort the variable names and put them back to string\n",
    "    v = ''.join(sorted(variableNames))\n",
    "  \n",
    "    if v != 'xy' and v != 'xyz':\n",
    "        raise ValueError(\"variableNames accepts only format like 'xy' or 'xyz'. \\\n",
    "It specifies which axis will be looked for duplicates.\")\n",
    "    \n",
    "    # check for the index values to be nice and in order\n",
    "    if list(data.index.values) != list(range(data.shape[0])):\n",
    "        raise ValueError(\"The index values of the data frame is not in order!.\")\n",
    "  \n",
    "    # initialize a counter\n",
    "    duplicateCounter = 0\n",
    "  \n",
    "    # initialize a data structure to store the indices\n",
    "    duplicates = []\n",
    "  \n",
    "    # specify the columns\n",
    "    cols = list(v)\n",
    "  \n",
    "    # make data a numpy array\n",
    "    #data = data[cols]\n",
    "  \n",
    "    # init previous line\n",
    "    prevLine = data.iloc[0]\n",
    "  \n",
    "    #for i in range(1, num_rows):\n",
    "    for i, row in data.iloc[1:].iterrows():\n",
    "    \n",
    "        #if np.all( prevLine == data.loc[[i]] ):\n",
    "        if prevLine[cols].equals(row[cols]):\n",
    "    \n",
    "            # increment a counter in case of duplicate\n",
    "            duplicateCounter += 1\n",
    "      \n",
    "            # add the indices to a data structure\n",
    "            duplicates.append((i-1, i))\n",
    "    \n",
    "        # keep the previous line to compare in the next iteration\n",
    "        prevLine = data.iloc[i]\n",
    "  \n",
    "    if removeDuplicates:\n",
    "      \n",
    "        indicesToKill = []\n",
    "    \n",
    "        for tup in duplicates:\n",
    "    \n",
    "            i, j = tup\n",
    "    \n",
    "            if data.iloc[i]['millis'] <= data.iloc[j]['millis']:\n",
    "        \n",
    "                #\n",
    "                indicesToKill.append(j)\n",
    "    \n",
    "        # drop the specified indices\n",
    "        data = data.drop(data.index[indicesToKill])\n",
    "    \n",
    "        data.index = range(data.shape[0])\n",
    "  \n",
    "        return data, duplicateCounter, duplicates, indicesToKill\n",
    "\n",
    "    return data, duplicateCounter, duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolation\n",
    "\n",
    "For the purpose of interpolation there will be two functions are involved. First we will be using a g-h filter to construct a new lock out of the user's lock, and then by superimposing this lock to the original one we can find the points for which there is a lot of disparity. Then we check the neighbours of that point and if their distance is too high in both cases then it means that this is a crazy point.\n",
    "\n",
    "The values of g and h that we found to be working best for our case are 0.9 in both cases. We found that by trial and error. Also, the way we combined interpolation and the filter was only to fix one point, then we run the filter again, on the corrected data and see if we find another point. That is because once we find a crazy point, it takes a while to adjust and in that little while, more points can easily be considered as crazy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothingGH(data, g=0.25, h=0.25):\n",
    "    \"\"\" Applies a simple g-h filter. The end index (end_idx) is included in the analysis. \"\"\"\n",
    "    \n",
    "    axis = ['x', 'y', 'z']\n",
    "    \n",
    "    df = pd.DataFrame(data.loc[:, axis])\n",
    "\n",
    "    # make an estimate according to the first three rows\n",
    "    x_est = df.loc[:2, axis].sum().div(3)\n",
    "            \n",
    "    # initialize delta\n",
    "    delta_xyz = [0, 0, 0]\n",
    "    \n",
    "    for i, row in df.loc[3:df.index.values[-2], axis].iterrows():\n",
    "        \n",
    "        # prediction step\n",
    "        x_pred = x_est + delta_xyz           # prediction estimate\n",
    "        delta_xyz = [0, 0, 0]                # reset delta\n",
    "\n",
    "        # update step\n",
    "        residual = row - x_pred              # residual calculation\n",
    "        delta_xyz = delta_xyz + h * residual # update delta\n",
    "        x_est = x_pred + g * residual        # update estimate\n",
    "        df.loc[i, axis] = x_est              # update data frame\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_two_smallest_indices(numbers, i, sp):\n",
    "    \"\"\" Finds the two smallest numbers in a list and their indices. Then, according to the \n",
    "    index of the row (i), and the space factor (sp) which defines how many points backwards will be\n",
    "    checked, the new index is being set. \"\"\"\n",
    "    m1, m2 = float('inf'), float('inf')\n",
    "    i1, i2 = 0, 0\n",
    "    for idx, x in enumerate(numbers):\n",
    "        if x <= m1:\n",
    "            m1, m2 = x, m1\n",
    "            i1, i2 = idx, i1\n",
    "        elif x < m2:\n",
    "            m2 = x\n",
    "            i2 = idx\n",
    "    return i+i1-sp, i+i2-sp\n",
    "\n",
    "\n",
    "def checkForTwoOffsInARow(A, B, C, D):\n",
    "    \"\"\" Finds whether there are two off values in a row and returns a boolean \"\"\"\n",
    "    \n",
    "    AB = np.sqrt(np.sum((B-A)**2))   # first vector distance\n",
    "    BC = np.sqrt(np.sum((C-B)**2))   # second vector distance\n",
    "    CD = np.sqrt(np.sum((D-C)**2))   # third vector distance\n",
    "    \n",
    "    if np.mean([AB, CD]) < 2*BC:\n",
    "        return True # one off value\n",
    "    else:\n",
    "        return False # two off values\n",
    "    \n",
    "\n",
    "def logPointToDict(d, num_points, i, i1, i2, cc):\n",
    "    \"\"\" Logs the point to dictionary \"\"\"\n",
    "    \n",
    "    d['point'+str(num_points+1)] = {'index': i, \n",
    "                                    'closest 2 indices(smooth_data)':(i1, i2),\n",
    "                                    'color code': cc}\n",
    "\n",
    "def findFromSmooth(row, data_smooth, axis_compare, sp, i):\n",
    "    \"\"\"  \"\"\"\n",
    "    # find the nearby points in the smoothened lock\n",
    "    nearby_points_smooth = [data_smooth.loc[j, axis_compare] for j in range(i-sp, i+sp)]\n",
    "\n",
    "    # find the distance between the rows and nearby rows\n",
    "    distances = np.sum((nearby_points_smooth - row.values)**2, axis=1)\n",
    "\n",
    "    # Find the indices of the two smallest points\n",
    "    point1_idx, point2_idx = find_two_smallest_indices(distances, i, sp)\n",
    "\n",
    "    # make linear space in between them for each axis\n",
    "    x_lin_space = np.linspace(data_smooth.loc[point1_idx, 'x'],\n",
    "                              data_smooth.loc[point2_idx, 'x'], 15)\n",
    "    y_lin_space = np.linspace(data_smooth.loc[point1_idx, 'y'], \n",
    "                              data_smooth.loc[point2_idx, 'y'], 15)\n",
    "\n",
    "    # combine the linear spaces\n",
    "    lin_space = np.transpose(np.array([x_lin_space, y_lin_space]))\n",
    "\n",
    "    # get the distances of all those points\n",
    "    distances_two = np.sqrt(np.sum((lin_space - row.values)**2, axis=1))\n",
    "\n",
    "    return min(distances_two), point1_idx, point2_idx\n",
    "\n",
    "\n",
    "\n",
    "def interpolate(data_interp, data_smooth, interpolation_points, skip_indices,\n",
    "                gh_threshold=20, interpolation_threshold=25, verbose=False, sp=3):\n",
    "    \"\"\" Interpolates on data, according to data_smooth and returns a new data structure. \n",
    "    - sp, is the space that is checked around an index\"\"\"\n",
    "        \n",
    "    axis_compare = ['x', 'y']                                         # keeps comparisons in 'x' and 'y'\n",
    "    df = pd.DataFrame(data_interp.loc[:, ['x', 'y', 'z', 'millis']])  # makes a new df to work with\n",
    "    found_one = False                                                 # control variable to control stopping\n",
    "    last_index = list(data_interp.index.values)[-6]                   # last point that will be checked\n",
    "    \n",
    "    # go through every point in the base dataset\n",
    "    for i, row in df.loc[sp+1:last_index, axis_compare].iterrows():\n",
    "        \n",
    "        if i in skip_indices:\n",
    "            continue\n",
    "        \n",
    "        # find the smallest distance between the point in the row and other points\n",
    "        smallest_distance, point1_idx, point2_idx = findFromSmooth(row, data_smooth, \n",
    "                                                                   axis_compare, sp, i)\n",
    "        \n",
    "        # if the difference is too large consider interpolation\n",
    "        if smallest_distance > gh_threshold:\n",
    "            \n",
    "            if found_one:\n",
    "                return df, True\n",
    "            \n",
    "            found_one = True\n",
    "            \n",
    "            # find the number of points to name the next point\n",
    "            num_points = len(interpolation_points.items())\n",
    "\n",
    "            # find whethere there are two points in a row that need to be interpolated\n",
    "            one_point_off = checkForTwoOffsInARow(df.loc[i-1, axis_compare], \n",
    "                                                  df.loc[i, axis_compare],\n",
    "                                                  df.loc[i+1, axis_compare],\n",
    "                                                  df.loc[i+2, axis_compare])\n",
    "            \n",
    "            if one_point_off:\n",
    "            \n",
    "                # find the new point that will occur after the interpolation\n",
    "                new_point = np.mean([df.loc[i-1, axis_compare], df.loc[i+1, axis_compare]], axis=0)\n",
    "            \n",
    "                # find the difference of the two points\n",
    "                interpolation_diff = np.sqrt(np.sum((df.loc[i, axis_compare] - new_point)**2, axis=0))\n",
    "                \n",
    "                if interpolation_diff > interpolation_threshold:\n",
    "                \n",
    "                    # interpolate\n",
    "                    df.loc[i, axis_compare] = new_point\n",
    "                    \n",
    "                    # log the value to the dictionary\n",
    "                    logPointToDict(interpolation_points, num_points, i, point1_idx, point2_idx, 'r')\n",
    "                        \n",
    "                else:\n",
    "\n",
    "                    # log the value to the dictionary\n",
    "                    logPointToDict(interpolation_points, num_points, \n",
    "                                   i, point1_idx, point2_idx, 'grey')\n",
    "                        \n",
    "                # mark the index so we dont go over it again\n",
    "                skip_indices.append(i)\n",
    "\n",
    "            else: # case of: one_point_off==False\n",
    "                \n",
    "                # take one third of the distance to use it for interpolation\n",
    "                a_third = (df.loc[i+2, axis_compare] - df.loc[i-1, axis_compare]) / 3\n",
    "                \n",
    "                # find the new point that will occur after the interpolation\n",
    "                new_point_one = df.loc[i-1, axis_compare] + a_third\n",
    "            \n",
    "                # find the new point that will occur after the interpolation\n",
    "                new_point_two = df.loc[i-1, axis_compare] + 2 * a_third\n",
    "            \n",
    "                # interpolate first point\n",
    "                df.loc[i, axis_compare] = new_point_one\n",
    "                \n",
    "                # log the value to the dictionary\n",
    "                logPointToDict(interpolation_points, num_points, i, point1_idx, point2_idx, 'r')\n",
    "                    \n",
    "                # interpolate second point\n",
    "                df.loc[i+1, axis_compare] = new_point_two\n",
    "                \n",
    "                # log the value to the dictionary\n",
    "                logPointToDict(interpolation_points, num_points+1, i+1, point2_idx, point2_idx+1, 'r')\n",
    "                    \n",
    "                # mark the index so we dont go over it again\n",
    "                skip_indices.append(i)\n",
    "                skip_indices.append(i+1)\n",
    "\n",
    "    return df, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre-Processing function and Keeping logs\n",
    "\n",
    "In the next cell there is a bundle pre-processing function. In some cases though, the files were too small or too bad to compute the PoIs and therefore some attention has also been paid to that in order to avoid crushing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data,\n",
    "                trimming_factor_front_mm=20,\n",
    "                trimming_factor_back_mm=0,\n",
    "                flat_back_trim=50):\n",
    "    \"\"\" This is a function that bundles-up the pre-processing functions. \"\"\"\n",
    "    \n",
    "    img_quality_good = True\n",
    "    \n",
    "    front_chop = 0\n",
    "    \n",
    "    duplicate_count = 0\n",
    "    \n",
    "    if data.shape[0] > 55:\n",
    "    \n",
    "        # step 1: reset coordinates\n",
    "        data = resetXY(data)\n",
    "\n",
    "        # step 2: chop sides\n",
    "        data, front_chop, back_chop = chopSides(data, \n",
    "                                                tFront=trimming_factor_front_mm,\n",
    "                                                tBack=trimming_factor_back_mm,\n",
    "                                                flatChopBack=flat_back_trim)\n",
    "                                \n",
    "    else:\n",
    "        \n",
    "        img_quality_good = False\n",
    "        \n",
    "        print(\"The data has too few rows (only {}) to remove duplicates or do any kind of chopping.\"\n",
    "              .format(data.shape[0]))\n",
    "    \n",
    "    if data.shape[0] > 15:\n",
    "        \n",
    "        # check and remove duplicates\n",
    "        data, duplicate_count, _, _ = checkForDuplicates(data, removeDuplicates=True)\n",
    "        \n",
    "        # smoothen in allowed keys\n",
    "        data_smooth = smoothingGH(data, g=0.9, h=0.9)\n",
    "\n",
    "        data_first_smooth = pd.DataFrame(data_smooth.loc[:, :]) # initialize and keep first smooth\n",
    "        data_interp = pd.DataFrame(data.loc[:, :])              # make it again for plotting the pure\n",
    "        \n",
    "        corrections_remaining = True\n",
    "\n",
    "        interpolation_points = {}\n",
    "\n",
    "        skip_indices = []\n",
    "\n",
    "        while corrections_remaining:\n",
    "            \n",
    "            # interpolate data according to data_smooth\n",
    "            data_interp, corrections_remaining = interpolate(data_interp,\n",
    "                                                             data_smooth,\n",
    "                                                             interpolation_points,\n",
    "                                                             skip_indices,\n",
    "                                                             gh_threshold=3,#.2, \n",
    "                                                             verbose=True)\n",
    "            \n",
    "            if corrections_remaining:\n",
    "\n",
    "                # run smoothing again on interpolated data\n",
    "                data_smooth = smoothingGH(data_interp, g=0.9, h=0.9)\n",
    "                \n",
    "    else:\n",
    "            \n",
    "        img_quality_good = False\n",
    "\n",
    "        print(\"The data after chopping has too few rows (only {}) for any meaningful processing.\"\n",
    "              .format(data.shape[0]))\n",
    "\n",
    "        dd = pd.DataFrame({'x': [1,4,2.5,1,4],\n",
    "                           'y': [1,4,2.5,4,1], \n",
    "                           'z': [1,1,1,1,1], \n",
    "                           'millis': [1,2,3,4,5]})\n",
    "        \n",
    "        # if too few points return data three times\n",
    "        return dd, dd, dd, {}, front_chop, duplicate_count, img_quality_good \n",
    "    \n",
    "    # that is what gets return if all goes well\n",
    "    return data, data_first_smooth, data_interp, interpolation_points, front_chop, duplicate_count, img_quality_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and Select the Locks\n",
    "\n",
    "In order to visualize the locks we will have them plotted and use user input Y/N for each image. All the Locks of a person will be visualized in a for loop and the loop will pause until input. In addition to that we want to produce some images that can be used for documentation purposes, that are without the title etc.\n",
    "\n",
    "In the next cells we will add the plotting function for the selection loop, as well as the functions for the saved image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCE(df, idxD, idxB, idxF, verbose=False):\n",
    "    \"\"\" Finds the C and E indices, based on B, D and F points. \"\"\"\n",
    "    \n",
    "    # take care of bugs\n",
    "    if idxB==0 or idxF==0:\n",
    "        print(\"There is an error: Points B or F are set to 0 which is an unrealistice value -> because of that, C and E are both set to 0.\")\n",
    "        return 0, 0\n",
    "    \n",
    "    # find index C\n",
    "    idxC = df['x'][idxB:idxD].idxmin()\n",
    "    \n",
    "    # find index E\n",
    "    idxE = df['x'][idxD:idxF].idxmax()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The C index is {}.\".format(idxC))\n",
    "        print(\"The E index is {}.\".format(idxE))\n",
    "        \n",
    "    return idxC, idxE\n",
    "\n",
    "\n",
    "def findF(df, idxG, idxD, verbose=False):\n",
    "    \"\"\" Finds the index of point F, based on the gradual change of the movement\n",
    "    (capture by percenting change). The calculations happen on around one third \n",
    "    of the shape.\"\"\"\n",
    "    \n",
    "    # make a dataframe that records the change in x and y values in percentages\n",
    "    dpct = df['x'][idxD:].pct_change(periods=1, fill_method='pad', limit=1, freq=None)\n",
    "    \n",
    "    # fill the NaN values\n",
    "    dpct = dpct.fillna(value=0)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # find the first index, coming from D, that has negative change\n",
    "        idxFirstNeg = dpct[dpct<0].head(1).index.values[0]\n",
    "    \n",
    "        # initialize index F\n",
    "        idxF = df['x'][idxFirstNeg:idxG].idxmin()\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        print(\"ERROR -> There is a problem with finding index F (probably no negative value between D to G)\")\n",
    "        return 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The F index is {}.\".format(idxF))\n",
    "        \n",
    "    return idxF\n",
    "\n",
    "\n",
    "def findB(df, idxA, idxD, verbose=False):\n",
    "    \"\"\" Finds the index of point B, based on the gradual change of the movement\n",
    "    (capture by percenting change). The calculations happen on around one third \n",
    "    of the shape.\"\"\"\n",
    "    \n",
    "    # make a dataframe that records the change in x and y values in percentages\n",
    "    dpct = df['x'][:idxD].pct_change(periods=1, fill_method='pad', limit=1, freq=None)\n",
    "    \n",
    "    # fill the NaN values\n",
    "    dpct = dpct.fillna(value=0)\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # find the first index, in the range from A to D, that has negative change\n",
    "        idxLastNeg = dpct[dpct<0].tail(1).index.values[0]\n",
    "        \n",
    "        # initialize index F\n",
    "        idxB = df['x'][idxA:idxLastNeg].idxmax()\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print(\"ERROR -> There is a problem with finding index B (probably no negative value between A to D)\")\n",
    "        return 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The B index is {}.\".format(idxB))\n",
    "        \n",
    "    return idxB\n",
    "\n",
    "\n",
    "def findG(df, verbose=False):\n",
    "    \"\"\" Finds the index of point G of the lock, by the maximun values of its two\n",
    "    columns x and y. \"\"\"\n",
    "  \n",
    "    # initialize dummy dataframe\n",
    "    dummy = pd.DataFrame(columns=['y_rev', 'x and y_rev'])\n",
    "  \n",
    "    # find the max value in the y axis\n",
    "    max_y = df['y'].max()\n",
    "    \n",
    "    if verbose:\n",
    "        print('The max value in y axis of the plot is {}.'.format(max_y))\n",
    "  \n",
    "    # make a reversed(y) column\n",
    "    dummy['y_rev'] = df['y'].apply(lambda x: max_y - x)\n",
    "\n",
    "    # make the column we extract G's index from\n",
    "    dummy['x and y_rev'] = df['x'].add(dummy['y_rev'])\n",
    "\n",
    "    # get the index of G\n",
    "    idxG = dummy['x and y_rev'].idxmax()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"The index G is... {}.\".format(idxG))\n",
    "\n",
    "    return idxG\n",
    "\n",
    "\n",
    "def findIndices(data):\n",
    "    \"\"\" Finds the indices of the Points of Interest in the lock. This function\n",
    "    workds under the assumption that the start of the axis is near point A. \"\"\"\n",
    "    \n",
    "    # find the indexes for A and H (both bottom left A starts the shape and H ends it) and G (bottom right)\n",
    "    iA, iH, iG = 0, -1, findG(data)\n",
    "    \n",
    "    # index for D (highest point)\n",
    "    iD = data['y'].idxmax()\n",
    "    \n",
    "    # find the index for B (neck left)\n",
    "    iB = findB(data, iA, iD)\n",
    "    \n",
    "    # find the index for F (neck right)\n",
    "    iF = findF(data, iG, iD)\n",
    "    \n",
    "    # find the indices for E (circle right) and F (neck right)\n",
    "    iC, iE = findCE(data, iD, iB, iF)\n",
    "        \n",
    "    return iA, iB, iC, iD, iE, iF, iG, iH\n",
    "\n",
    "\n",
    "def findPoints(data, iA, iB, iC, iD, iE, iF, iG, iH, keepTimestamp=False):\n",
    "    \"\"\" Finds the points in the dataframe according the indices \"\"\"\n",
    "    \n",
    "    if keepTimestamp: # find ABCDEFGH while keeping 'millis' column\n",
    "        \n",
    "        A = data.iloc[iA]  # bottom left (starting point)\n",
    "        B = data.iloc[iB]  # neck left\n",
    "        C = data.iloc[iC]  # head left\n",
    "        D = data.iloc[iD]  # highest point\n",
    "        E = data.iloc[iE]  # head right\n",
    "        F = data.iloc[iF]  # neck right\n",
    "        G = data.iloc[iG]  # bottom right\n",
    "        H = data.iloc[iH]  # bottom left (ending point)\n",
    "            \n",
    "        return A, B, C, D, E, F, G, H\n",
    "        \n",
    "    else: # find ABCDEFGH while dropping 'millis' column\n",
    "        \n",
    "        A = data.iloc[iA][:3]  # bottom left (starting point)\n",
    "        B = data.iloc[iB][:3]  # neck left\n",
    "        C = data.iloc[iC][:3]  # head left\n",
    "        D = data.iloc[iD][:3]  # highest point\n",
    "        E = data.iloc[iE][:3]  # head right\n",
    "        F = data.iloc[iF][:3]  # neck right\n",
    "        G = data.iloc[iG][:3]  # bottom right\n",
    "        H = data.iloc[iH][:3]  # bottom left (ending point)\n",
    "            \n",
    "        return A, B, C, D, E, F, G, H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# plotting style\n",
    "plt.style.use('seaborn-deep')\n",
    "\n",
    "def findBorders(X, Y, Z):\n",
    "    \"\"\" Finds the plotting dimensions for the box that ensure that all sides are \n",
    "    the the same size and the real dimensions lock can fit in \"\"\"\n",
    "  \n",
    "    # mins, maxes, differences, and medians for every axis\n",
    "    mins = [np.min(X), np.min(Y), np.min(Z)]\n",
    "    maxs = [np.max(X), np.max(Y), np.max(Z)]\n",
    "    diff = [I-J for I,J in zip(maxs, mins)]\n",
    "    mids = [(I/2)+J for I,J in zip(diff, mins)]\n",
    "    # get the highest difference\n",
    "    bigMax = max(diff)\n",
    "    # calculated half the side of the plotting box\n",
    "    gap = (bigMax/2) + bigMax*0.05\n",
    "    # return the plotting box dimensions\n",
    "    return [mids[0]-gap, mids[1]-gap, mids[2]-gap, mids[0]+gap, mids[1]+gap, mids[2]+gap]\n",
    "\n",
    "\n",
    "def simplePlotter(data, data_first_smooth, data_interp, true_world_scale=True, first_img='', \n",
    "                  second_img='', image_title='', interpolation_points=None, show_image=True, \n",
    "                  PoIs=True, img_quality_good=True):\n",
    "    \"\"\" This is the basic plotting function that plots an instance. Its parameters \n",
    "    are explained as follows:\n",
    "    - data: should be a loaded data frame (eg. pandas df)\n",
    "    - trueWorldScale: plots will be in a box with all sides the same size\n",
    "    - imageTitle: the name of the image\n",
    "    - showImage: controls whether images will be outputed at the ipython console\n",
    "    - panoramic: if true the angle is set to give panoramic view\"\"\"\n",
    "    \n",
    "    # set the graph\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(12)\n",
    "    fig.set_figheight(6)\n",
    "\n",
    "    # set axes\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    ax2 = fig.add_subplot(122, projection='3d', sharex=ax1)\n",
    "    # plot axes\n",
    "    ax1.plot(data['x'], data['y'], data['z'])\n",
    "    ax1.plot(data_first_smooth['x'], data_first_smooth['y'], data_first_smooth['z'], \n",
    "             alpha=0.6, lw=1, c='g')\n",
    "    ax2.plot(data_interp['x'], data_interp['y'], data_interp['z'])\n",
    "    \n",
    "    if interpolation_points:\n",
    "        for _, point in interpolation_points.items():\n",
    "            # get the info of the interpolation point\n",
    "            idx = point['index']\n",
    "            cc = point['color code']\n",
    "            # plot the interpolation point in ax1\n",
    "            ax1.scatter(data['x'][idx], data['y'][idx], data['z'][idx], color=cc, s=20, alpha=0.55)\n",
    "            if cc=='r':\n",
    "                # plot the interpolation point in ax2\n",
    "                ax2.scatter(data_interp['x'][idx], data_interp['y'][idx], data_interp['z'][idx], \n",
    "                            color=cc, s=20, alpha=0.55)\n",
    "            # plot the points used to find it\n",
    "            p1, p2 = point['closest 2 indices(smooth_data)']\n",
    "            ax1.scatter(data_first_smooth['x'][p1], \n",
    "                        data_first_smooth['y'][p1], \n",
    "                        data_first_smooth['z'][p1], color=\"g\", s=20, alpha=0.35)\n",
    "            ax1.scatter(data_first_smooth['x'][p2], \n",
    "                        data_first_smooth['y'][p2], \n",
    "                        data_first_smooth['z'][p2], color=\"g\", s=20, alpha=0.35)\n",
    "   \n",
    "    if PoIs and img_quality_good:\n",
    "    \n",
    "        # finds the indices for all points\n",
    "        iA, iB, iC, iD, iE, iF, iG, iH = findIndices(data_interp)\n",
    "\n",
    "        # finds the points \n",
    "        A, B, C, D, E, F, G, H = findPoints(data_interp, iA, iB, iC, iD, iE, iF, iG, iH)\n",
    "    \n",
    "        # plot points of interest\n",
    "        ax2.scatter(*A, color=\"g\", s=100, alpha=0.65)  #A (starting point)\n",
    "        ax2.scatter(*B, color=\"y\", s=100, alpha=0.65)  #B\n",
    "        ax2.scatter(*C, color=\"y\", s=100, alpha=0.65)  #C\n",
    "        ax2.scatter(*D, color=\"y\", s=100, alpha=0.65)  #D\n",
    "        ax2.scatter(*E, color=\"y\", s=100, alpha=0.65)  #E\n",
    "        ax2.scatter(*F, color=\"y\", s=100, alpha=0.65)  #F\n",
    "        ax2.scatter(*G, color=\"y\", s=100, alpha=0.65)  #G\n",
    "        ax2.scatter(*H, color=\"r\", s=100, alpha=0.65)  #H (ending point)\n",
    "\n",
    "    # adjust the space in-between the axes\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)    \n",
    "    \n",
    "    # set titles\n",
    "    ax1.set_title(first_img)\n",
    "    ax2.set_title(second_img)\n",
    "    # figure title\n",
    "    plt.suptitle(image_title, fontsize=12)\n",
    "      \n",
    "    # label axes\n",
    "    ax1.set_xlabel('x', linespacing=1)\n",
    "    ax1.set_ylabel('y', linespacing=1)\n",
    "    ax1.set_zlabel('z', linespacing=1)\n",
    "    ax2.set_xlabel('x', linespacing=1)\n",
    "    ax2.set_ylabel('y', linespacing=1)\n",
    "    ax2.set_zlabel('z', linespacing=1)\n",
    "  \n",
    "    # set the axis according to world scale\n",
    "    if true_world_scale:\n",
    "        limits = findBorders(data['x'],data['y'],data['z'])\n",
    "        #print(limits[3] - limits[0], limits[4] - limits[1], limits[5] - limits[2])\n",
    "        ax1.set_xlim3d(limits[0], limits[3])\n",
    "        ax1.set_ylim3d(limits[1], limits[4])\n",
    "        ax1.set_zlim3d(limits[2], limits[5])\n",
    "        ax2.set_xlim3d(limits[0], limits[3])\n",
    "        ax2.set_ylim3d(limits[1], limits[4])\n",
    "        ax2.set_zlim3d(limits[2], limits[5])\n",
    "  \n",
    "    # rotate for desired angle\n",
    "    ax1.view_init(elev=90., azim=270)\n",
    "    ax2.view_init(elev=90., azim=270)\n",
    "    \n",
    "    #ax1.tick_params(axis='x', labelrotation=45, direction='out')\n",
    "    #fig.autofmt_xdate()\n",
    "    #fig.tight_layout()\n",
    "    ax1.locator_params(tight=True, axis='x', nbins=6)\n",
    "    ax1.locator_params(tight=True, axis='y', nbins=6)\n",
    "    ax1.locator_params(tight=True, axis='z', nbins=6)\n",
    "    ax2.locator_params(tight=True, axis='x', nbins=6)\n",
    "    ax2.locator_params(tight=True, axis='y', nbins=6)\n",
    "    ax2.locator_params(tight=True, axis='z', nbins=6)\n",
    "    plt.tight_layout()\n",
    "   \n",
    "    # show the figure into the plt console\n",
    "    if show_image:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_show(data, image_title, true_world_scale=True):\n",
    "    \"\"\" This functions is to make and save the plot of only the raw data. \"\"\"\n",
    "    \n",
    "    # set the graph\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(6)\n",
    "    \n",
    "    # set axes\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # plot axes\n",
    "    ax.plot(data['x'], data['y'], data['z'])\n",
    "              \n",
    "    # label axes\n",
    "    ax.set_xlabel('x', linespacing=1)\n",
    "    ax.set_ylabel('y', linespacing=1)\n",
    "    ax.set_zlabel('z', linespacing=1)\n",
    "  \n",
    "    # set the axis according to world scale\n",
    "    if true_world_scale:\n",
    "        limits = findBorders(data['x'],data['y'],data['z'])\n",
    "        #print(limits[3] - limits[0], limits[4] - limits[1], limits[5] - limits[2])\n",
    "        ax.set_xlim3d(limits[0], limits[3])\n",
    "        ax.set_ylim3d(limits[1], limits[4])\n",
    "        ax.set_zlim3d(limits[2], limits[5])\n",
    "  \n",
    "    # rotate for desired angle\n",
    "    ax.view_init(elev=90., azim=270)\n",
    "    \n",
    "    ax.locator_params(tight=True, axis='x', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='y', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='z', nbins=6)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def saveRawDataPlot(data, saving_path, image_title, true_world_scale=True):\n",
    "    \"\"\" This functions is to make and save the plot of only the raw data. \"\"\"\n",
    "    \n",
    "    # set the graph\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(6)\n",
    "    \n",
    "    # set axes\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # plot axes\n",
    "    ax.plot(data['x'], data['y'], data['z'])\n",
    "              \n",
    "    # label axes\n",
    "    ax.set_xlabel('x', linespacing=1)\n",
    "    ax.set_ylabel('y', linespacing=1)\n",
    "    ax.set_zlabel('z', linespacing=1)\n",
    "  \n",
    "    # set the axis according to world scale\n",
    "    if true_world_scale:\n",
    "        limits = findBorders(data['x'],data['y'],data['z'])\n",
    "        #print(limits[3] - limits[0], limits[4] - limits[1], limits[5] - limits[2])\n",
    "        ax.set_xlim3d(limits[0], limits[3])\n",
    "        ax.set_ylim3d(limits[1], limits[4])\n",
    "        ax.set_zlim3d(limits[2], limits[5])\n",
    "  \n",
    "    # rotate for desired angle\n",
    "    ax.view_init(elev=90., azim=270)\n",
    "    \n",
    "    ax.locator_params(tight=True, axis='x', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='y', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='z', nbins=6)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig(saving_path + image_title + '_raw' + '.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "def saveRawAndSmoothDataPlot(data, data_first_smooth, saving_path, image_title, \n",
    "                             interpolation_points, true_world_scale=True):\n",
    "    \"\"\" This functions is to make and save the plot of the raw and smoothened data with their \n",
    "    PoIs. \"\"\"\n",
    "    \n",
    "    # set the graph\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(6)\n",
    "    \n",
    "    # set axes\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # plot axes\n",
    "    ax.plot(data['x'], data['y'], data['z'])\n",
    "    ax.plot(data_first_smooth['x'], data_first_smooth['y'], data_first_smooth['z'], \n",
    "            alpha=0.6, lw=1, c='g')\n",
    "    \n",
    "    if interpolation_points:\n",
    "        for _, point in interpolation_points.items():\n",
    "            # get the info of the interpolation point\n",
    "            idx = point['index']\n",
    "            cc = point['color code']\n",
    "            # plot the interpolation point in ax1\n",
    "            ax.scatter(data['x'][idx], data['y'][idx], data['z'][idx], color=cc, s=20, alpha=0.55)\n",
    "            # find the points used to calculate the interpolation\n",
    "            p1, p2 = point['closest 2 indices(smooth_data)']\n",
    "            # plot those points\n",
    "            ax.scatter(data_first_smooth['x'][p1], \n",
    "                       data_first_smooth['y'][p1], \n",
    "                       data_first_smooth['z'][p1], color=\"g\", s=20, alpha=0.35)\n",
    "            ax.scatter(data_first_smooth['x'][p2], \n",
    "                       data_first_smooth['y'][p2], \n",
    "                       data_first_smooth['z'][p2], color=\"g\", s=20, alpha=0.35)\n",
    "          \n",
    "    # label axes\n",
    "    ax.set_xlabel('x', linespacing=1)\n",
    "    ax.set_ylabel('y', linespacing=1)\n",
    "    ax.set_zlabel('z', linespacing=1)\n",
    "  \n",
    "    # set the axis according to world scale\n",
    "    if true_world_scale:\n",
    "        limits = findBorders(data['x'],data['y'],data['z'])\n",
    "        #print(limits[3] - limits[0], limits[4] - limits[1], limits[5] - limits[2])\n",
    "        ax.set_xlim3d(limits[0], limits[3])\n",
    "        ax.set_ylim3d(limits[1], limits[4])\n",
    "        ax.set_zlim3d(limits[2], limits[5])\n",
    "  \n",
    "    # rotate for desired angle\n",
    "    ax.view_init(elev=90., azim=270)\n",
    "    \n",
    "    ax.locator_params(tight=True, axis='x', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='y', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='z', nbins=6)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig(saving_path + image_title + '_raw-smooth' + '.png')\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "def saveInterpDataPlot(data_interp, saving_path, image_title, interpolation_points, \n",
    "                       img_quality_good, true_world_scale=True, PoIs=True):\n",
    "    \"\"\" This functions is to make and save the plot of the interpolated data with their PoIs. \"\"\"\n",
    "    \n",
    "    # set the graph\n",
    "    fig = plt.figure()\n",
    "    fig.set_figwidth(6)\n",
    "    fig.set_figheight(6)\n",
    "    \n",
    "    # set axes\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    # plot axes\n",
    "    ax.plot(data_interp['x'], data_interp['y'], data_interp['z'])\n",
    "    \n",
    "    if interpolation_points:\n",
    "        for _, point in interpolation_points.items():\n",
    "            # get the info of the interpolation point\n",
    "            idx = point['index']\n",
    "            cc = point['color code']\n",
    "            # plot the interpolation point in ax1\n",
    "            if cc=='r':\n",
    "                # plot the interpolation point in ax2\n",
    "                ax.scatter(data_interp['x'][idx], data_interp['y'][idx], data_interp['z'][idx], \n",
    "                           color=cc, s=20, alpha=0.55)\n",
    "   \n",
    "    if PoIs and img_quality_good:\n",
    "    \n",
    "        # finds the indices for all points\n",
    "        iA, iB, iC, iD, iE, iF, iG, iH = findIndices(data_interp)\n",
    "\n",
    "        # finds the points \n",
    "        A, B, C, D, E, F, G, H = findPoints(data_interp, iA, iB, iC, iD, iE, iF, iG, iH)\n",
    "    \n",
    "        # plot points of interest\n",
    "        ax.scatter(*A, color=\"g\", s=100, alpha=0.65)  #A (starting point)\n",
    "        ax.scatter(*B, color=\"y\", s=100, alpha=0.65)  #B\n",
    "        ax.scatter(*C, color=\"y\", s=100, alpha=0.65)  #C\n",
    "        ax.scatter(*D, color=\"y\", s=100, alpha=0.65)  #D\n",
    "        ax.scatter(*E, color=\"y\", s=100, alpha=0.65)  #E\n",
    "        ax.scatter(*F, color=\"y\", s=100, alpha=0.65)  #F\n",
    "        ax.scatter(*G, color=\"y\", s=100, alpha=0.65)  #G\n",
    "        ax.scatter(*H, color=\"r\", s=100, alpha=0.65)  #H (ending point)\n",
    "\n",
    "    # label axes\n",
    "    ax.set_xlabel('x', linespacing=1)\n",
    "    ax.set_ylabel('y', linespacing=1)\n",
    "    ax.set_zlabel('z', linespacing=1)\n",
    "  \n",
    "    # set the axis according to world scale\n",
    "    if true_world_scale:\n",
    "        limits = findBorders(data_interp['x'],data_interp['y'],data_interp['z'])\n",
    "        #print(limits[3] - limits[0], limits[4] - limits[1], limits[5] - limits[2])\n",
    "        ax.set_xlim3d(limits[0], limits[3])\n",
    "        ax.set_ylim3d(limits[1], limits[4])\n",
    "        ax.set_zlim3d(limits[2], limits[5])\n",
    "  \n",
    "    # rotate for desired angle\n",
    "    ax.view_init(elev=90., azim=270)\n",
    "    \n",
    "    ax.locator_params(tight=True, axis='x', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='y', nbins=6)\n",
    "    ax.locator_params(tight=True, axis='z', nbins=6)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig(saving_path + image_title + '_interpolated' + '.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Lock Selection Process\n",
    "\n",
    "The following cell requires user input, the locks are being displayed, and the user has to write 'y' or 'n' (only 1 letter). If 'y', a new file that has been undergone pre-processing will be outputed, and some logs will be kept. If 'n', then the image will be left out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 78 images that are kept out of the selection process.\n",
      "There are 15 images that are dropped out of the selection process.\n"
     ]
    }
   ],
   "source": [
    "logs = ['' for x in range(26)]\n",
    "\n",
    "lock_file_names_kept, lock_file_names_dropped = [], []\n",
    "front_chops_per_lock, duplicates_per_lock, interpolations_per_lock = [], [], []\n",
    "\n",
    "decided = False\n",
    "\n",
    "back_chop = 50\n",
    "\n",
    "for file in lock_file_names:\n",
    "    \n",
    "    # load the file\n",
    "    data_raw = pd.read_csv(source_path+file)\n",
    "    \n",
    "    # do the pre-processing steps to the data and pass the variables for logging\n",
    "    data_raw, data_first_smooth, data_interp, interpolation_points, \\\n",
    "    front_chop , duplicate_count, img_quality_good = processData(data_raw, flat_back_trim=back_chop)\n",
    "    #print(data_interp[:3])\n",
    "    # show raw&smooth:left - interpolated:right\n",
    "    simplePlotter(data_raw, data_first_smooth, data_interp, \n",
    "                  first_img='Non-interpolated data & on top of first Smoothing',\n",
    "                  second_img='Interpolated data (with highlights) & PoIs', \n",
    "                  interpolation_points=interpolation_points,\n",
    "                  image_title=file, \n",
    "                  img_quality_good=img_quality_good)\n",
    "    \n",
    "    # loop infinately until a button is pressed\n",
    "    while not decided:\n",
    "        \n",
    "        # get the key\n",
    "        key = str(input(\"Is it a lock (Y/N)? \"))\n",
    "        # re evaluate while\n",
    "        decided = True if key is 'Y' or key is 'y' or key is 'N' or key is 'n' else False\n",
    "    \n",
    "    # reset the control key for next loop\n",
    "    decided = False\n",
    "    \n",
    "    if key is 'Y' or key is 'y':\n",
    "        \n",
    "        # add file to kept\n",
    "        lock_file_names_kept.append(file)\n",
    "\n",
    "        inter_counter = 0\n",
    "\n",
    "        for _, point in interpolation_points.items():\n",
    "            \n",
    "            if point['color code']=='r':\n",
    "            \n",
    "                # increment the count counter if the point is red color coded\n",
    "                inter_counter += 1\n",
    "\n",
    "        # add the interpolated points counter\n",
    "        interpolations_per_lock.append(inter_counter)\n",
    "        \n",
    "        # add the front chop to the front chops\n",
    "        front_chops_per_lock.append(front_chop)\n",
    "    \n",
    "        # add the duplicate count to the duplicates\n",
    "        duplicates_per_lock.append(duplicate_count)\n",
    "    \n",
    "        # make logs\n",
    "        logs.append('In {}, the frontchop was {} rows and the backchop {}; there were also found {} duplicates, and {} points were interpolated.'\n",
    "                    .format(file, front_chop, back_chop, duplicate_count, inter_counter))\n",
    "\n",
    "        # save the file into new location\n",
    "        data_interp.to_csv(target_path+file, index=False)\n",
    "        \n",
    "        # save the images\n",
    "        saveRawDataPlot(data_raw, img_saving_path_accepted, file)\n",
    "        saveRawAndSmoothDataPlot(data_raw, data_first_smooth, img_saving_path_accepted, \n",
    "                                 file, interpolation_points)\n",
    "        saveInterpDataPlot(data_interp, img_saving_path_accepted, file, \n",
    "                           interpolation_points, img_quality_good)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # add file to dropped\n",
    "        lock_file_names_dropped.append(file)\n",
    "\n",
    "        # save the images\n",
    "        saveRawDataPlot(data_raw, img_saving_path_rejected, file)\n",
    "        saveRawAndSmoothDataPlot(data_raw, data_first_smooth, img_saving_path_rejected, \n",
    "                                 file, interpolation_points)\n",
    "        saveInterpDataPlot(data_interp, img_saving_path_rejected, file, \n",
    "                           interpolation_points, img_quality_good)\n",
    "    \n",
    "    # clear all output for next plot\n",
    "    clear_output()\n",
    "    \n",
    "    #break\n",
    "\n",
    "print(\"There are {} images that are kept out of the selection process.\"\n",
    "      .format(len(lock_file_names_kept)))\n",
    "print(\"There are {} images that are dropped out of the selection process.\"\n",
    "      .format(len(lock_file_names_dropped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop out some logs for the process\n",
    "\n",
    "Some stuff have been saved temporarily - like the name of the files - and some others can be quickly computed that show some statistics regarding the front chop and the duplicates. By making a logs csv we maintain all that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log file is as follows:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      Some stats for the distributions of the front_...\n",
       "1                    front chop mean: 18.564102564102566\n",
       "2                      front chop std: 3.168716247338545\n",
       "3                                front chop median: 18.0\n",
       "4                                     front chop min: 12\n",
       "5                                     front chop max: 33\n",
       "6                                             - - - - - \n",
       "7                     duplicates mean: 4.987179487179487\n",
       "8                      duplicates std: 3.894419379203745\n",
       "9                                 duplicates median: 4.0\n",
       "10                                     duplicates min: 1\n",
       "11                                    duplicates max: 20\n",
       "12                                            - - - - - \n",
       "13              interpolations mean: 0.34615384615384615\n",
       "14                interpolations std: 0.5734940832396165\n",
       "15                            interpolations median: 0.0\n",
       "16                                 interpolations min: 0\n",
       "17                                 interpolations max: 2\n",
       "18     ----------------------------------------------...\n",
       "19     The criterion to keep or drop a lock was its '...\n",
       "20                              number of locks kept: 78\n",
       "21                           number of locks dropped: 15\n",
       "22                                            - - - - - \n",
       "23     From the files that have been kept, in 23 out ...\n",
       "24     ----------------------------------------------...\n",
       "25     The front-back chops and the duplicates in eac...\n",
       "26     In Giorgos_M19_f__0.csv, the frontchop was 19 ...\n",
       "27     In Giorgos_M19_f__1.csv, the frontchop was 16 ...\n",
       "28     In Giorgos_M19_f__2.csv, the frontchop was 17 ...\n",
       "29     In Giorgos_M19_f__3.csv, the frontchop was 17 ...\n",
       "30     In Giorgos_M19_f__4.csv, the frontchop was 18 ...\n",
       "31     In Giorgos_M19_f__5.csv, the frontchop was 16 ...\n",
       "32     In Giorgos_M19_f__6.csv, the frontchop was 19 ...\n",
       "33     In Giorgos_M19_f__7.csv, the frontchop was 18 ...\n",
       "34     In Giorgos_M19_f__8.csv, the frontchop was 22 ...\n",
       "35     In Giorgos_M19_f__9.csv, the frontchop was 17 ...\n",
       "36     In Giorgos_M19_f__10.csv, the frontchop was 18...\n",
       "37     In Giorgos_M19_f__11.csv, the frontchop was 20...\n",
       "38     In Giorgos_M19_f__12.csv, the frontchop was 18...\n",
       "39     In Giorgos_M19_f__13.csv, the frontchop was 15...\n",
       "40     In Giorgos_M19_f__14.csv, the frontchop was 19...\n",
       "41     In Giorgos_M19_f__15.csv, the frontchop was 18...\n",
       "42     In Giorgos_M19_f__16.csv, the frontchop was 23...\n",
       "43     In Giorgos_M19_f__17.csv, the frontchop was 23...\n",
       "44     In Giorgos_M19_f__18.csv, the frontchop was 19...\n",
       "45     In Giorgos_M19_f__19.csv, the frontchop was 18...\n",
       "46     In Giorgos_M19_f__20.csv, the frontchop was 21...\n",
       "47     In Giorgos_M19_f__21.csv, the frontchop was 19...\n",
       "48     In Giorgos_M19_f__23.csv, the frontchop was 21...\n",
       "49     In Giorgos_M19_f__24.csv, the frontchop was 21...\n",
       "50     In Giorgos_M19_f__25.csv, the frontchop was 33...\n",
       "51     In Giorgos_M19_f__26.csv, the frontchop was 20...\n",
       "52     In Giorgos_M19_f__27.csv, the frontchop was 16...\n",
       "53     In Giorgos_M19_f__28.csv, the frontchop was 18...\n",
       "54     In Giorgos_M19_f__29.csv, the frontchop was 16...\n",
       "55     In Giorgos_M19_f__30.csv, the frontchop was 16...\n",
       "56     In Giorgos_M19_f__31.csv, the frontchop was 31...\n",
       "57     In Giorgos_M19_f__32.csv, the frontchop was 16...\n",
       "58     In Giorgos_M19_f__33.csv, the frontchop was 21...\n",
       "59     In Giorgos_M19_f__34.csv, the frontchop was 17...\n",
       "60     In Giorgos_M19_f__35.csv, the frontchop was 19...\n",
       "61     In Giorgos_M19_f__36.csv, the frontchop was 22...\n",
       "62     In Giorgos_M19_f__37.csv, the frontchop was 18...\n",
       "63     In Giorgos_M19_f__38.csv, the frontchop was 20...\n",
       "64     In Giorgos_M19_f__39.csv, the frontchop was 20...\n",
       "65     In Giorgos_M19_f__40.csv, the frontchop was 22...\n",
       "66     In Giorgos_M19_f__41.csv, the frontchop was 17...\n",
       "67     In Giorgos_M19_f__43.csv, the frontchop was 20...\n",
       "68     In Giorgos_M19_f__45.csv, the frontchop was 16...\n",
       "69     In Giorgos_M19_f__48.csv, the frontchop was 18...\n",
       "70     In Giorgos_M19_f__50.csv, the frontchop was 12...\n",
       "71     In Giorgos_M19_f__51.csv, the frontchop was 18...\n",
       "72     In Giorgos_M19_f__52.csv, the frontchop was 17...\n",
       "73     In Giorgos_M19_f__53.csv, the frontchop was 19...\n",
       "74     In Giorgos_M19_f__54.csv, the frontchop was 17...\n",
       "75     In Giorgos_M19_f__55.csv, the frontchop was 15...\n",
       "76     In Giorgos_M19_f__56.csv, the frontchop was 24...\n",
       "77     In Giorgos_M19_f__57.csv, the frontchop was 16...\n",
       "78     In Giorgos_M19_f__58.csv, the frontchop was 18...\n",
       "79     In Giorgos_M19_f__59.csv, the frontchop was 17...\n",
       "80     In Giorgos_M19_f__60.csv, the frontchop was 17...\n",
       "81     In Giorgos_M19_f__61.csv, the frontchop was 24...\n",
       "82     In Giorgos_M19_f__63.csv, the frontchop was 15...\n",
       "83     In Giorgos_M19_f__64.csv, the frontchop was 18...\n",
       "84     In Giorgos_M19_f__65.csv, the frontchop was 15...\n",
       "85     In Giorgos_M19_f__66.csv, the frontchop was 16...\n",
       "86     In Giorgos_M19_f__67.csv, the frontchop was 15...\n",
       "87     In Giorgos_M19_f__68.csv, the frontchop was 17...\n",
       "88     In Giorgos_M19_f__69.csv, the frontchop was 19...\n",
       "89     In Giorgos_M19_f__70.csv, the frontchop was 17...\n",
       "90     In Giorgos_M19_f__71.csv, the frontchop was 18...\n",
       "91     In Giorgos_M19_f__72.csv, the frontchop was 20...\n",
       "92     In Giorgos_M19_f__74.csv, the frontchop was 13...\n",
       "93     In Giorgos_M19_f__75.csv, the frontchop was 20...\n",
       "94     In Giorgos_M19_f__78.csv, the frontchop was 18...\n",
       "95     In Giorgos_M19_f__81.csv, the frontchop was 20...\n",
       "96     In Giorgos_M19_f__82.csv, the frontchop was 19...\n",
       "97     In Giorgos_M19_f__83.csv, the frontchop was 18...\n",
       "98     In Giorgos_M19_f__84.csv, the frontchop was 18...\n",
       "99     In Giorgos_M19_f__85.csv, the frontchop was 17...\n",
       "100    In Giorgos_M19_f__87.csv, the frontchop was 16...\n",
       "101    In Giorgos_M19_f__90.csv, the frontchop was 21...\n",
       "102    In Giorgos_M19_f__91.csv, the frontchop was 18...\n",
       "103    In Giorgos_M19_f__92.csv, the frontchop was 18...\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_files_kept = len(lock_file_names_kept)\n",
    "\n",
    "num_kept_and_interpolated = np.count_nonzero(interpolations_per_lock)\n",
    "\n",
    "perc_interpolated = num_kept_and_interpolated / num_files_kept * 100\n",
    "\n",
    "logs[0] = \"Some stats for the distributions of the front_chop, the duplicates and the number of interpolations per lock in the files that have been kept are:\"\n",
    "logs[1] = \"front chop mean: {}\".format(np.mean(front_chops_per_lock))\n",
    "logs[2] = \"front chop std: {}\".format(np.std(front_chops_per_lock))\n",
    "logs[3] = \"front chop median: {}\".format(np.median(front_chops_per_lock))\n",
    "logs[4] = \"front chop min: {}\".format(np.min(front_chops_per_lock))\n",
    "logs[5] = \"front chop max: {}\".format(np.max(front_chops_per_lock))\n",
    "logs[6] = \" - - - - - \"\n",
    "logs[7] = \"duplicates mean: {}\".format(np.mean(duplicates_per_lock))\n",
    "logs[8] = \"duplicates std: {}\".format(np.std(duplicates_per_lock))\n",
    "logs[9] = \"duplicates median: {}\".format(np.median(duplicates_per_lock))\n",
    "logs[10] = \"duplicates min: {}\".format(np.min(duplicates_per_lock))\n",
    "logs[11] = \"duplicates max: {}\".format(np.max(duplicates_per_lock))\n",
    "logs[12] = \" - - - - - \"\n",
    "logs[13] = \"interpolations mean: {}\".format(np.mean(interpolations_per_lock))\n",
    "logs[14] = \"interpolations std: {}\".format(np.std(interpolations_per_lock))\n",
    "logs[15] = \"interpolations median: {}\".format(np.median(interpolations_per_lock))\n",
    "logs[16] = \"interpolations min: {}\".format(np.min(interpolations_per_lock))\n",
    "logs[17] = \"interpolations max: {}\".format(np.max(interpolations_per_lock))\n",
    "logs[18] = \"---------------------------------------------------------------------------------------\"\n",
    "logs[19] = \"The criterion to keep or drop a lock was its 'lockyness', no matter some sporadic offshoot values \\\n",
    "that are clear glitches and can be interpolated.\"\n",
    "logs[20] = \"number of locks kept: {}\".format(num_files_kept)\n",
    "logs[21] = \"number of locks dropped: {}\".format(len(lock_file_names_dropped))\n",
    "logs[22] = \" - - - - - \"\n",
    "logs[23] = \"From the files that have been kept, in {} out of the {} cases ({:.2f}%) we interpolated one or more points.\".format(num_kept_and_interpolated, num_files_kept, perc_interpolated)\n",
    "logs[24] = \"---------------------------------------------------------------------------------------\"\n",
    "logs[25] = \"The front-back chops and the duplicates in each file separately are:\"\n",
    "\n",
    "# put the list into a series\n",
    "logs = pd.Series(logs, index=range(len(logs)))\n",
    "\n",
    "print(\"The log file is as follows:\\n\")\n",
    "\n",
    "display(logs)\n",
    "\n",
    "# drop the csv\n",
    "logs.to_csv(target_path+'logs/logs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
